---
title: "Упражнение  9"
author: "Никишов Дмитрий"
date: "12 05 2021"
output: html_document
---

Необходимо построить модель на основе SVM для указанной в варианте зависимой переменной.

Данные взять из упражнения №3.

Для модели:

1 Отложить 25% наблюдений в тестовую выборку (ядро генератора случайных чисел указано в варианте к упражнению №3).

2 На обучающей выборке (оставшихся 75% наблюдений) сравнить несколько видов ядер SVM по точности модели (AUC) методом сеточного поиска.

3 Для оптимальной формы ядерной функции на обучающей выборке подобрать оптимальное значение настроечных параметров по минимальной ошибке с перекрёстной проверкой (функция tune).

4 Подогнать лучшую модель на всей обучающей выборке. Построить ROC-кривую и рассчитать матрицу неточностей, чувствительность и специфичность.

5 Сделать прогноз по лучшей модели на тестовую выборку, оценить его качество точность по матрице неточностей, чувствительность и специфичность, построить ROC-кривую.

6 Сравнить результаты, которые дал SVM, с результатами, полученными в упражнении 3. Какой из методов оказался лучше?


## Вариант 21 (3)

 - Ядро для set.seed() - 123.

  - Данные: *PimaIndiansDiabetes{mlbench}* - случаи диабета у женщин индейского племени Пима.

 - Зависимая переменная: *diabetes* (*pos* - наличие признака, *neg* - отсутствие).

 - Объясняющие переменные: Все остальные.

 - Методы: *Логистическая регрессия, QDA*.

Пакеты:

```{r setup, include=FALSE}
library('e1071')     # SVM
library('ROCR')      # ROC-кривые
library('mlbench')   # Данные PimaIndiansDiabetes
library('ISLR')
library('GGally')
library('MASS')


knitr::opts_chunk$set(echo = TRUE)
```

Зададим ядро генератора случайных чисел и объём обучающей выборки.

```{r}
my.seed <- 123        # Ядро генерации
train.percent <- 0.75 # Доля обучающей выборки
```

- Исходные данные: *PimaIndiansDiabetes{mlbench}* - случаи диабета у женщин индейского племени Пима

```{r}
data(PimaIndiansDiabetes)
head(PimaIndiansDiabetes)
dim(PimaIndiansDiabetes)

diabetes <- rep(0, length(PimaIndiansDiabetes$diabetes)) # Создание вектора diabetes
PimaIndiansDiabetes <- cbind(PimaIndiansDiabetes, diabetes)        # Присоединение diabetes к фрейму PimaIndiansDiabetes

# Замена в переменной diabetes: если diabetes = pos означает наличие признака (1), neg - отсутствие(0)
for(i in 1:length(PimaIndiansDiabetes$diabetes)) {if (PimaIndiansDiabetes$diabetes[i] == "pos") {PimaIndiansDiabetes$diabetes[i] == "neg"}}

# Определение долей
table(PimaIndiansDiabetes$diabetes) / sum(table(PimaIndiansDiabetes$diabetes))
```

Доля наименьшего класса, в данном случае 0.349, это ошибка нулевого классификатора: если бы мы прогнозировали diabets = pos  для всех наблюдений, ровно в такой доле случаев мы бы ошиблись. Точность моделей целесообразно будет сравнивать с этой величиной

# Сеточный поиск

```{r}
#Отбираем наблюдения в обучающую выборку 
set.seed(my.seed)
inTrain <- sample(seq_along(PimaIndiansDiabetes$diabetes),
                  nrow(PimaIndiansDiabetes)*train.percent)
xtrain <- PimaIndiansDiabetes[inTrain, c(-9, -10)]
xtest <- PimaIndiansDiabetes[-inTrain, c(-9, -10)]
ytrain <- PimaIndiansDiabetes[inTrain, 9]
ytest <- PimaIndiansDiabetes[-inTrain, 9]

# Обучающая выборка
dat <- data.frame(x = xtrain, y = as.factor(ytrain))

# Тестовые данные
dat.te <- data.frame(x = xtest, y = as.factor(ytest))

# Параметры алгоритма
kernel.grid <- c('linear', 'polynomial')
cost.grid <- seq(1, 20, by = 0.5)

AUC <- matrix(0, length(kernel.grid), length(cost.grid))
colnames(AUC) <- paste0('cost = ', cost.grid)
rownames(AUC) <- paste0('kernel = ', kernel.grid)

# SVM 
for (i in 1:length(kernel.grid)) {
    print(paste0('Starting ', kernel.grid[i], ' kernel'))
    for (j in 1:length(cost.grid)) {
        out <- svm(y ~ ., data = dat, kernel = kernel.grid[i], 
                   cost = cost.grid[j])
        # Прогноз на тестовой выборке
        pred.te <- predict(out, newdata = dat.te)
        # Матрица неточностей
        tbl <- table(pred.te, dat.te$y)
        AUC[i, j] <- sum(diag(tbl)) / sum(tbl)
    }
}

round(AUC, 3)
```

Из полученных результатов видно, что оптимальной формой ядерной функции будет линейная модель.


# Оптимальное значение настроечного параметра

```{r}
# Классификатор на опорных векторах с линейной границей
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 10, cale = FALSE)

# Список опорных векторов
svmfit$index

# Сводка по модели
summary(svmfit)
```
Делаем перекрёстную проверку, изменяя штраф (аргумент cost)
```{r}
# 
set.seed(my.seed)
tune.out <- tune(svm, y ~ ., data = dat, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.1, 1, 5, 10, 100)))
summary(tune.out)
```
# Лучшая модель на всей обучающей выборке

```{r}
# Лучшая модель -- с минимальной ошибкой
bestmod <- tune.out$best.model
summary(bestmod)
```

```{r}
# Делаем прогноз по лучшей модели
ypred_tr <- predict(bestmod, dat)

# Матрица неточностей
tbl1 <- table(Predicts = ypred_tr, Fact = dat$y)
tbl1

# Чувствительность
TPR <- round(tbl1[2,2]/sum(tbl1[2,]),3)  
TPR

# Специфичность
SPC <- round(tbl1[1,1]/sum(tbl1[1,]),3)  
SPC
```

```{r}
# Функция построения ROC-кривой: pred -- прогноз, truth -- факт
rocplot <- function(pred, truth, ...){
    predob = prediction(pred, truth)
    perf = performance(predob, "tpr", "fpr")
    plot(perf,...)}

# Оптимальная модель
svmfit.opt <- svm(y ~ ., data = dat, kernel = "linear",  cost = 0.01, probability = T)

# Матрица неточностей на обучающей (p = 0.01)
table(Predicts = predict(svmfit.opt, dat), 
             Fact = dat$y)

# Прогноз вероятностей, на основе которых присваивается класс
fitted.prob <- predict(svmfit.opt, dat, type = "prob",  probability = TRUE)
fitted.prob <- attr(fitted.prob, "probabilities")[, 2]

# График для обучающей выборки
# ROC-кривая для первой модели
rocplot(fitted.prob, dat[, "y"], main = "Training Data")
# Прямая случайного классификатора
abline(a = 0, b = 1, lty = 3, lwd = 2)
```

# Лучшая модель на тестовой выборке

```{r}
# График для тестовой выборки
fitted.prob <- predict(svmfit.opt, dat.te, type = "prob",
                       probability = TRUE)
fitted.prob <- attr(fitted.prob, "probabilities")[, 2]

# Матрица неточностей на тестовой (p = 0.01)
tbl2 <- table(Predicts = predict(svmfit.opt, dat.te), Fact = dat.te$y)
tbl2

# Точность
ACC <- round(sum(diag(tbl2))/sum(tbl2),3)  
ACC

# Чувствительность
TPR <- round(tbl2[2,2]/sum(tbl2[2,]),3)  
TPR

# Специфичность
SPC <- round(tbl2[1,1]/sum(tbl2[1,]),3)  
SPC

# ROC-кривая для тестовой выборки
rocplot(fitted.prob, dat.te$y, main = "Test Data")
# Прямая случайного классификатора
abline(a = 0, b = 1, lty = 3, lwd = 2)
```

Как видно из графиков ROC-кривых, и для обучающей, и для тестовой выборок значение AUC более 0.5.

# Сравнение моделей (логистическая регрессия, LDA, SVM) на тестовой выборке

```{r}
# логистическая регрессия
model.logit <- glm(y ~ ., data = dat, family = 'binomial')
summary(model.logit)
```

```{r}

p.logit <- predict(model.logit, dat.te, 
                  type = 'response')

Forecast1 <- factor(ifelse(p.logit > 0.5, 2, 1),
                  levels = c(1, 2),
                  labels = c('neg', 'pos'))

# считаем 1-SPC и TPR для всех вариантов границы отсечения
x1 <- NULL    # для (1 - SPC)
y1 <- NULL    # для TPR

# заготовка под матрицу неточностей
# Заготовка под матрицу неточностей
tbl1 <- as.data.frame(matrix(rep(0, 4), 2, 2))
rownames(tbl1) <- c('fact.neg', 'fact.pos')
colnames(tbl1) <- c('predict.neg', 'predict.pos')

# цикл по вероятностям отсечения
for (p in seq(0, 1, length = 501)){
    # прогноз
    Forecast1 <- factor(ifelse(p.logit > p, 2, 1),
                        levels = c(1, 2),
                        labels = c('neg', 'pos'))

    # фрейм со сравнением факта и прогноза
    df.compare <- data.frame(Fact = dat.te$y, Forecast = Forecast1)

    # заполняем матрицу неточностей
    tbl1[1, 1] <- nrow(df.compare[df.compare$Fact == 'neg' & df.compare$Forecast == 'neg', ])
    tbl1[2, 2] <- nrow(df.compare[df.compare$Fact == 'pos' & df.compare$Forecast == 'pos', ])
    tbl1[1, 2] <- nrow(df.compare[df.compare$Fact == 'neg' & df.compare$Forecast == 'pos', ])
    tbl1[2, 1] <- nrow(df.compare[df.compare$Fact == 'pos' & df.compare$Forecast == 'neg', ])

    # считаем характеристики
    TPR <- tbl1[2, 2] / sum(tbl1[2, ])
    y1 <- c(y1, TPR)
    SPC <- tbl1[1, 1] / sum(tbl1[1, ])
    x1 <- c(x1, 1 - SPC)
    }



# LDA
model.lda <- lda(y ~ ., data = dat)

# Прогноз: вероятности принадлежности классу diabetes
p.lda <- predict(model.lda, dat.te, type = 'response')

x2 <- NULL    # для (1 - SPC)
y2 <- NULL    # для TPR

# Заготовка под матрицу неточностей
tbl2 <- as.data.frame(matrix(rep(0, 4), 2, 2))
rownames(tbl2) <- c('fact.neg', 'fact.pos')
colnames(tbl2) <- c('predict.neg', 'predict.pos')


# вектор вероятностей для перебора
#p.vector <- seq(0, 1, length = 501)

# цикл по вероятностям отсечения
for (p in seq(0, 1, length = 501)){
  # Прогноз
  Forecast2 <- factor(ifelse(p.lda$posterior[, 'pos'] > p, 2, 1),
                      levels = c(1, 2),
                      labels = c('neg', 'pos'))
  
  # фрейм со сравнением факта и прогноза
  df.compare <- data.frame(Fact = dat.te$y, Forecast = Forecast2)
  
  # Заполняем матрицу неточностей
  tbl2[1, 1] <- nrow(df.compare[df.compare$Fact == 'neg' & df.compare$Forecast == 'neg', ])
  tbl2[2, 2] <- nrow(df.compare[df.compare$Fact == 'pos' & df.compare$Forecast == 'pos', ])
  tbl2[1, 2] <- nrow(df.compare[df.compare$Fact == 'neg' & df.compare$Forecast == 'pos', ])
  tbl2[2, 1] <- nrow(df.compare[df.compare$Fact == 'pos' & df.compare$Forecast == 'neg', ])
  
  # Считаем характеристики
  TPR <- tbl2[2, 2] / sum(tbl2[2, ])
  y2 <- c(y2, TPR)
  SPC <- tbl2[1, 1] / sum(tbl2[1, ])
  x2 <- c(x2, 1 - SPC)
}




# Строим ROC-кривую
par(mar = c(5, 5, 1, 1))

# кривая (логистическая регрессия)
plot(x1, y1, type = 'l', col = 'blue', lwd = 3,
     xlab = '(1 - SPC)', ylab = 'TPR', 
     xlim = c(0, 1), ylim = c(0, 1), main = 'Тестовая выборка')

# кривая (LDA)
lines(x2, y2, type = 'l', col = 'red', lwd = 3)

# Кривая (SVM обр.)
rocplot(-fitted.prob, dat.te$y, add = T, col = 'green')

# Прямая случайного классификатора
abline(a = 0, b = 1, lty = 3, lwd = 2)

# Легенда
legend('bottomright', names <-  c('LDA', 'Лог. регр.', 'SVM'), lty = 1, col = c('blue', 'red', 'green'))
```

Сравнивая ROC-кривые, полученные на тестовой выборке, видно, что логистическая регрессия обладает большей предсказательной способностью, чем LDA и SVM.